<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8" />
    <title>Penguins vs Turtles</title>
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css"
      rel="stylesheet"
      integrity="sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM"
      crossorigin="anonymous"
    />
    <link rel="stylesheet" href="assets/css/style.css" />
  </head>
  <body>
    <div id="content">
      <article>
        <div class="col-md-3 col-sm-12">
          <h1 class="page-header">Penguins vs Turtles</h1>
        </div>

        <div class="container">
          <div class="row" class="col-md-12"></div>
            <div class="col-md-2 col-sm-0"></div>
            <div class="col-md-3 col-sm-12"> 
              <h1 class="page-header"></h1>
            </div>
        </div>
<br>
<br>
<br>
        <div class="jumbotron jumbotron-fluid">
          <div class="container" style="background: #ffff">
              <div class="mt-0">
                <h2>Infographic</h2>
                <div style='border-bottom:3px solid lightgrey;'></div>
              </div>
              <div style="margin-top: 10px; padding: 20px;">
                <!-- <img src="assets/images/me.jpeg" class="col-md-3 col-sm-12 img-fluid"> -->
                <img src="assets/images/infographic_v2.png">
              </div>
            </div>
          </div>
        

        <div class="jumbotron jumbotron-fluid">
          <div class="container" style="background: #ffff">
            <div class="mt-0">
              <h2>Introduction</h2>
              <div style="border-bottom: 3px solid lightgrey"></div>
            </div>
            <div style="margin-top: 10px; padding: 20px">
              <!-- <img src="assets/images/me.jpeg" class="col-md-3 col-sm-12 img-fluid"> -->
              <p>
                Given three distinct types of images; penguins, turtles, and
                strictly images that are landscapes. We wish to develop an
                algorithm that can distinguish between these three types of
                images. Currently, classification of images is most successful
                when utilizing Deep Learning, or Convolutional Neural Networks
                (CNN) as these models can identify meaningful transformations of
                images for us, rather than hand crafting these transformations
                from the images. Despite these models having remarkable success
                with images where the target is the singular subject. They tend
                to still have difficulties correctly classifying an image if the
                image has a lot of “clutter” or “noise” and can often fall short
                if there are not enough datapoints for the model to work with.
              </p>
            </div>
          </div>
        
        <div class="jumbotron jumbotron-fluid">
          <div class="container" style="background: #ffff">
              <div class="mt-0">
                <h2>Methodology</h2>
                <div style='border-bottom:3px solid lightgrey;'></div>
              </div>
              <div style="margin-top: 10px; padding: 20px;">
                <!-- <img src="assets/images/me.jpeg" class="col-md-3 col-sm-12 img-fluid"> -->
                <p>
                  Our data currently consists of 572 unique images. 500 of these images are used to train the machine learning model,
                  while the rest of the images are reserved for evaluating the trained model’s performance. Our dataset uses discrete 
                  categories to describe the target values, where 0 represents the background, 1 represents the penguins, and 2 represents 
                  the turtles. This indicates that our dataset is discrete.
                </p>
                <p> 
                  For cleaning up and standardizing our images we used the provided bounding boxes to reduce the image size. Taking a step further, 
                  we resized each image to 256 by 256 and 64 by 64 pixels. These actions served multiple purposes, including reducing noise, improving 
                  computational efficiency, and removing outliers. Each image was then placed into a dataframe where they could be further played with 
                  with functions we develpoed. These fucntions include: conversion and extraction of the bounding box, finding and applying contours, resizing, applying 
                  greyscale and preprocessing. Additionally, we may note that the finalized images were placed into a pkl file for ease of access and to 
                  reduce computation time.
                </p>
                <p>
                  For feature extraction, we ran several algorithms like Canny Edge detection, Histogram of Gradients, MobileNetV2 and ORB. Canny Edge 
                  Detection was specifically employed to detect edges of the turtle or penguin in the given image. Histogram of Gradients calculated the 
                  distribution of gradients within the image, generating a histogram based on the gradient values. Among these algorithms, the MobileNetV2 
                  gave the best results. On the other hand, the algorithm that gave us the worst results was the Oriented FAST and Rotated BRIEF(ORB), 
                  which is a combination of the FAST corner detector and the BRIEF descriptor. ORB detected key points and features in an image based on 
                  local intensity, subsequently computing binary descriptors to represent those features. To further reduce dimensionality and perform 
                  feature selection, we used PCA and Kernel PCA. 
                </p>
                <p>
                  Although CNN literature gave us promising outlook at the start, our current classifier’s result is quite underwhelming. Since our task of 
                  separating three different classes of images have been done numerous times, we decided to use pre-trained model after researching about our goal. 
                  There were several excellent choices, promising accuracy of at least 92%; the top contenders being a Sequential TensorFlow keras model and a 
                  ResNet model. Simplicity is key, so we first adopted the Sequential model.
                </p>
                <p>
                  Adam optimizer widely used in Deep Learning. Since it is an extension of stochastic gradient descent, it is excellent in helping us avoid 
                  getting stuck on local minima. For the compile function’s metrics, we use accuracy, which will allow keras to choose the best accuracy function 
                  for us. Below is a summary of our CNN model.
                </p>
                <img src="assets/images/cnn_model_summary.png">
              </div>
            </div>
        </div>
        <div class="jumbotron jumbotron-fluid">
          <div class="container" style="background: #ffff">
              <div class="mt-0">
                <h2>Results</h2>
                <div style='border-bottom:3px solid lightgrey;'></div>
              </div>
              <div style="margin-top: 10px; padding: 20px;">
                <p>
                  The following are selected plots: 
                </p>
                <figure>
                 <img src="assets/images/Canny_Edges_Features_PCA.jpg">
                <figcaption>Reducing dimensionality with PCA and applying Canny Edges delivered ambiguous results.</figcaption>
                </figure>
                <figure>     <img src="assets/images/HOG_Features_PCA.jpg">
                  <figcaption>With HOG Translation, we achieved a better result.</figcaption>
                </figure>

                 <figure>
                 <img src="assets/images/k_means_visualization.png" style="width: 1000px; height: 600;">
                 <figcaption>With K-Means, we are able to beautifully separate the turtles and penguins into separate clusters based on color value</figcaption>
                 </figure> 
                 <img src="assets/images/MobileNetV2_Features_PCA.jpg"> 
                 <img src="assets/images/cluster_gmm_edge_pca.jpg"> 
                 <img src="assets/images/cluster_gmm_edge_pca_metrics.jpg"> 
                 <img src="assets/images/cluster_gmm_hog_pca.jpg"> 
                 <img src="assets/images/cluster_gmm_hog_pca_metrics.jpg"> 
                 <img src="assets/images/cluster_gmm_mobilenet_bb_pca.jpg"> 
                 <figure>
                 <img src="assets/images/cluster_gmm_mobilenet_bb_pca_metrics.jpg"> 
                 <figcaption>We were able to get a good separation of the two animals using MobileNetV2, even with reduced dimensions. This method 
                   yields the best result, as reflected in the chart of the evaluation metrics</figcaption>
                </figure>
                 <img src="assets/images/cluster_kmean_edge_pca.jpg"> 
                 <img src="assets/images/cluster_kmean_hog_pca.jpg">
                 <img src="assets/images/cluster_kmean_hog_pca_metrics.jpg"> 
                 <img src="assets/images/cluster_kmean_mobilenet_bb_pca.jpg">
                 <img src="assets/images/cluster_kmean_mobilenet_bb_pca_metrics.jpg">
                <p> 
                  From the accuracy metric, we were able to record our model’s training and validation accuracy, which is reflected in the plot below.
                </p>
                <img src="assets/images/cnn_accuracy.png">
                <img src="assets/images/cnn_loss.png">
                <p>
                  Our classifier is highly volatile in its prediction accuracy, which is hovering around 40%. Further, the loss is too high. 
                  Which means we are making numerous mistakes. This is likely due to several factors:                
                </p>
                <ul>
                  <li>
                    As this is an image classifier, our model will do better with more image data. Five hundred is the minimum, and evidently 
                    it’s very little in practice.
                  </li>
                  <li>
                    The bounding boxes are too big compared to some zoomed out images of the animal.<a href="contact.html">Contact</a>
                  </li>
                  <li>
                    Our model needs more variation of images, further elaborated in the next paragraph.
                  </li>
                  <li>
                    The animals are evolved to blend in with their surrounding for survival. Hence, the confusion between animal and background 
                    image is high. This is reflected in a 2 class classifier between just the penguins and turtle (plot attached below). This model 
                    has much higher and stable accuracy.
                  </li>
                </ul>
                <img src="assets/images/cnn_twoclass_accuracy.png">
                <img src="assets/images/cnn_twoclass_loss.png">
              </div>
            </div>
        </div>
        <div class="jumbotron jumbotron-fluid">
          <div class="container" style="background: #ffff">
              <div class="mt-0">
                <h2>Discussion</h2>
                <div style='border-bottom:3px solid lightgrey;'></div>
              </div>
              <div style="margin-top: 10px; padding: 20px;">
                <p>
                  By resizing the images and strictly analyzing the background color, we were able to improve the differentiation
                  between images of turtles and penguins. When using edge detection, which helped represent features like the beaks, 
                  fins, and shells in the provided image did work so well as you can see in the above image. This suggested that the 
                  features between penguins and turtles might be too like rely solely on edge. Using Histogram of Gradients, we 
                  discovered penguins exhibit more uniform and gradual changes in gradients, while turtles had a more varied and 
                  complex gradient. Despite this information, a clear separation between the two classes was still not apparent. 
                  The most effective approach was the utilization of MobileNetV2 because it automatically distinguished specific 
                  patterns associated with turtles and penguins and used them as features. 
                </p>
                <p>
                  Due to the poor performance of edge detection and gradient distribution we decided to try incorporating these 
                  techniques with other ones like PCA. We also incorporated data augmentation, utilizing techniques such as funneling 
                  our images through a data frame and experimenting with trace breaks in code to explore alternative ideas.
                </p>
                <p>
                  During our unsupervised learning analysis, we used Clustering, K-means, Gaussian mixture Models (GMM), and DBSCAN. 
                  Evaluating the clustering results provided insights into the quality of the clusters. The Silhouette coefficient, which 
                  measures cluster separation, seemed relatively low for all charts, indicating that clusters were overlapping with each 
                  other. The DBI metric supports the presence of overlapping clusters with it being high around 0.85. Looking at the NMI 
                  metric, which quantifies the similarity between predicted clusters and true labels, the score was relatively low. This 
                  suggests that the clusters have a limited similarity and our cluster algorithms struggled to capture underlying patterns 
                  accurately. Finally, Folkes-Mallow, which is used to evaluate the similarities between predicted clusters and true labels, 
                  demonstrated higher scores. Comparing the performance between GMM and K-Means, it seemed like K-Means did much better 
                  with its NMI mostly being larger than all the algorithms except Histogram of Gradients. In summary, our analysis revealed 
                  the challenge of achieving distinct and well separated clusters due to overlapping patterns.
                </p> 
                <p>
                  As stated above in results, when looking at each alogrithm, MobileNetv2 yieled the best result when paired with PCA. 
                  Kmeans, HOG, and Canny Edges provided decent clustering but fall short, and that makes sense as MobileNetV2 is designed
                  to process a more complicated load of imformation. Each of the other three are simpler and lack the ability to recognize 
                  the complexity an image may present. Kmeans and HOG still performed decently with PCA, whereas Canny Edges was just ok as
                  it isn't as directly involved in that process. 
                </p>
                <p>
                  Given more time to work on this project, we would like to do more exploration on this data. More data preprocessing and 
                  augmentation. Simple techniques such as zooms, rotations and flips might make a huge difference. Grayscale conversion and 
                  image inversion will give us more information about our misclassification, as per our project midpoint feedback (was pixel intensity the key?).
                </p>
                <p>
                  In our efforts to improve our classifier, we would like to discover the more non-obvious features of our images and 
                  scrutinizing through the details in our data.Additionally, we would like to try comparing the sequential method. Which 
                  initially did not seem as appealing to us due to lower accuracy as described in the literature.
                </p>
              </div>
            </div>
        </div>
        <div class="jumbotron jumbotron-fluid">
          <div class="container" style="background: #ffff">
              <div class="mt-0">
                <h2>References</h2>
                <div style='border-bottom:3px solid lightgrey;'></div>
              </div>
              <div style="margin-top: 10px; padding: 20px;">
                <a href="https://www.kaggle.com/datasets/abbymorgan/penguins-vs-turtles"
                >Kaggle Turtles and Penguins Dataset</a
              >
              <br />
              <a
                href="https://ieeexplore.ieee.org/abstract/document/9132851?casa_token=GeeAyY3ANu0AAAAA:4df-4yo2tQKZr5yTgExPcTzbx4RzWSUkYqooTc7ygn71H5q7BOMXPrhDW9TG9X_Bmg0LgT8unQ"
                >Image Classification using SVM and CNN</a
              >
              <br />
              <a href="https://www.nature.com/articles/s41467-022-27980-y"
                >Perspectives in machine learning for wildlife conservation</a
              >
              </div>
            </div>
        </div>
      </article>
    </div>
    <footer>
      By Blake West, Derrick Yu, Eirene Lakshita, Lauren Fowler, John Binek
    </footer>
  </body>
</html>
