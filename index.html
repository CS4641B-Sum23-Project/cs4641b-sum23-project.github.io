<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8" />
    <title>Penguins vs Turtles</title>
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css"
      rel="stylesheet"
      integrity="sha384-9ndCyUaIbzAi2FUVXJi0CjmCapSmO7SnpJef0486qhLnuZ2cdeRhO02iuK6FUUVM"
      crossorigin="anonymous"
    />
    <link rel="stylesheet" href="assets/css/style.css" />
  </head>
  <body>
    <div id="content">
      <article>
        <div class="col-md-4 col-sm-10">
          <h1 class="page-header">Penguins vs Turtles</h1>
        </div>
<br>
<br>
<br>
        <div class="jumbotron jumbotron-fluid">
          <div class="container" style="background: #ffff">
              <div class="mt-0">
                <h2>Infographic</h2>
                <div style='border-bottom:3px solid lightgrey;'></div>
              </div>
              <div style="margin-top: 10px; padding: 20px;">
                <!-- <img src="assets/images/me.jpeg" class="col-md-3 col-sm-12 img-fluid"> -->
                <img src="assets/images/infographic_v2.png">
              </div>
            </div>
          </div>
        

        <div class="jumbotron jumbotron-fluid">
          <div class="container" style="background: #ffff">
            <div class="mt-0">
              <h2>Introduction</h2>
              <div style="border-bottom: 3px solid lightgrey"></div>
            </div>
            <div style="margin-top: 10px; padding: 20px">
              <!-- <img src="assets/images/me.jpeg" class="col-md-3 col-sm-12 img-fluid"> -->
              <p>
                Given three distinct types of images; penguins, turtles, and
                strictly images that are landscapes. We wish to develop an
                algorithm that can distinguish between these three types of
                images. Currently, classification of images is most successful
                when utilizing Deep Learning, or Convolutional Neural Networks
                (CNN) as these models can identify meaningful transformations of
                images for us, rather than hand crafting these transformations
                from the images. Despite these models having remarkable success
                with images where the target is the singular subject, they tend
                to still have difficulties correctly classifying an image if the
                image has a lot of “clutter” or “noise” and can often fall short
                if there are not enough datapoints for the model to work with.
              </p>
            </div>
          </div>
        
        <div class="jumbotron jumbotron-fluid">
          <div class="container" style="background: #ffff">
              <div class="mt-0">
                <h2>Methodology</h2>
                <div style='border-bottom:3px solid lightgrey;'></div>
              </div>
              <div style="margin-top: 10px; padding: 20px;">
                <!-- <img src="assets/images/me.jpeg" class="col-md-3 col-sm-12 img-fluid"> -->
                <p>
                  Our data currently consists of 857 unique images. 749 of these images are used to train the machine learning model,
                  while the rest of the images are reserved for evaluating the trained model’s performance. Our dataset uses discrete 
                  categories to describe the target values, where 1 represents a penguin, 2 represents a turtle, and 3 represents 
                  the background.
                </p>
                <p> 
                  To clean up and standardize our images we used the provided bounding boxes to reduce the image size. We reduced image size even further for specific purposes, e.g. clustering using   
                  KMeans and training our CNN, by scaling our images to 256x256 and 64x64 pixels when necessary.
                  In doing so, we reduced noise, improved computational efficiency, and removed outliers.  
                  Each image was then placed into a dataframe where they could be further played with 
                  with functions we develpoed. These fucntions include: conversion and extraction of the bounding box, finding and applying contours, resizing, applying 
                  greyscale and preprocessing. Additionally, we may note that the finalized images were placed into a pkl file for ease of access and to 
                  reduce computation time.
                </p>
                <p>
                  For evaluation metrics we used accuracy and confusion matrix. The reason we used it is because confusion matrices work well with models with multiple classes. For example, in our project we have three classes: penguins, turtles, and backgrounds. Furthermore, they provide information on true positives, true negatives, and false positives, and false negatives. As for accuracy, it is simply used to serve as a baseline metric to assess a model’s performance.  
                </p>
                <p>
                  We tested several algorithms for feature extraction such as Canny Edge detection, Histogram of Gradients, MobileNetV2 and ORB. Canny Edge Detection was specifically employed to detect edges of the turtle or penguin in the given image. Histogram of Gradients calculated the distribution of gradients within the image, generating a histogram based on the gradient values. Among these algorithms, the MobileNetV2 gave the best results. On the other hand, the algorithm that gave us the worst results was the Oriented FAST and Rotated BRIEF(ORB), which is a combination of the FAST corner detector and the BRIEF descriptor. ORB detected key points and features in an image based on local intensity, subsequently computing binary descriptors to represent those features. To further reduce dimensionality and perform feature selection, we used PCA and Kernel PCA.
                </p>
              </div>
            </div>
        </div>
        <div class="jumbotron jumbotron-fluid">
          <div class="container" style="background: #ffff">
              <div class="mt-0">
                <h2>Results</h2>
                <div style='border-bottom:3px solid lightgrey;'></div>
                <h3> Unsupervised </h3>
              </div>
              <div style="margin-top: 10px; padding: 20px;">
                 <figure>
                 <img class="graph" src="assets/images/Kmeans Centroids/KMeans Centroids.png" style="width: 1000px; height: 600;">
                 <figcaption>
We utilized KMeans to visualize and cluster our normalized pixel values. In the above graph, you can see that there is some overlap between the three clusters, indicating that there will be some confusion using unsupervised learning algorithms to separate our data. Ideally, these clusters would be completely separated. 
                 </figcaption>
                 </figure> 
                 
                 <figure>
                  <img class="graph" src="assets/images/HOG/PCA.png" style="width: 1000px; height:600";>
                  <img class="graph" src="assets/images/HOG/Kernel PCA/rbf.png" style="width: 1000px; height:600";>
                  <figcaption>
                    In the two above images, we utilized PCA and KPCA to visualize features extracted by HOG.  These graphs indicate a high degree of overlap in features even after reducing the dimensionality of the data.  
                  </figcaption>
                 </figure>

                 <figure>
                  <img class="graph" src="assets/images/MobileNet Bounding Box/PCA.png" style="width: 1000px; height: 600;">
                  <img class="graph" src="assets/images/MobileNet Bounding Box/Kernel PCA/rbf.png" style="width: 1000px; height: 600;">
                  <img class="graph" src="assets/images/MobileNet No Bounding Box/PCA/PCA.png" style="width: 1000px; height: 600;">
                  <img class="graph" src="assets/images/MobileNet No Bounding Box/Kernel PCA/rbf.png" style="width: 1000px; height: 600;">
                  <figcaption>
                   We tested MobileNetV2 with and without bounding boxes. PCA performed poorly on both versions of our data, with there being an almost perfect overlap between penguins and turtles. We saw better separation when using a rbf kernel for KPCA. Surprisingly, there is better separation of the features without using bounding boxes.  
 
                  </figcaption>
                 </figure>

                 <img class="graph" src="assets/images/MobileNetV2_Features_PCA.jpg">
                 <img class="graph" src="assets/images/cluster_gmm_edge_pca.jpg">
                 <img class="graph" src="assets/images/cluster_gmm_edge_pca_metrics.jpg">
                 <img class="graph" src="assets/images/cluster_gmm_hog_pca.jpg">
                 <img class="graph" src="assets/images/cluster_gmm_hog_pca_metrics.jpg">
                 <img class="graph" src="assets/images/cluster_gmm_mobilenet_bb_pca.jpg">
                 <img class="graph" src="assets/images/cluster_kmean_edge_pca.jpg">
                 <img class="graph" src="assets/images/cluster_kmean_hog_pca.jpg">
                 <img class="graph" src="assets/images/cluster_kmean_hog_pca_metrics.jpg">
                 <img class="graph" src="assets/images/cluster_kmean_mobilenet_bb_pca.jpg">
                 <img class="graph" src="assets/images/cluster_kmean_mobilenet_bb_pca_metrics.jpg">
                 
                 <div class="jumbotron jumbotron-fluid">
                  <div class="container" style="background: #ffff">
                      <div class="mt-0">
                        <h2>Results</h2>
                        <div style='border-bottom:3px solid lightgrey;'></div>
                        <h3> Supervised </h3>
                      </div>
                      <div style="margin-top: 10px; padding: 20px;">
                        <h3> SVM</h3>
                        <figure>
                          <img class="graph" src="assets/images/SVM/rbf_05.png">
                          <img class="graph" src="assets/images/SVM/rbf_075.png">
                          <figcaption>
                            SVM performed poorly on features extracted from MobileNetV2 using our dataset without bounding boxes. Our best performance came from using alpha values of 0.5 and 0.75 to reduce the number of filters while extracting MobileNetV2 features. Both alpha values had the same classification accuracy of 45.37% with separate spreads of misclassified images. Future work to try and improve classification accuracy would include extracting images of animals using the bounding boxes and tuning hyperparameters of the SVM model.  
         
                          </figcaption>
                         </figure>
                         <p></p>
                         <div style='border-bottom:3px solid lightgrey;'></div>
                         <p></p>
                         <h3> Decision Tree and Random Forest</h3>
                         <figure>
                          <img class="graph" src="assets/images/all_acc_vs_max_depth.jpg">
                          <figcaption>
                            <p>
                              Looking at the above plot you can see that the average accuracy for most of the decision trees is higher than the random forests, which is an unexpected outcome. This is mostly likely because of the limited datapoints in our dataset, and it is not prone to overfitting. Furthermore, the features extracted from our images play a significant role in determining accuracy, hence why we go forward with MobileNet features. Because the average accuracy vs maximum depth looks constant after 6-7 maximum depths, tuning the maximum depth does not seem necessary at an initial glance.
                            </p>
                            <p>
                              Despite the Decision Trees accuracy on average being higher than Random Forests. The best set of hyperparameters for MobileNet Bounding Box features with a Decisions Tree is a maximum depth of 11, and maximum features of 32.5%. With these parameters, the decision tree performs with an accuracy of 88.9%. Random Forests, however, perform better with an accuracy of 96.3% with 15 estimators, 15 maximum depth, and 65% maximum features. This coincides with our maximum depth graph for Random Forests, as the Random Forest bounding box features accuracy improves as we increase the maximum depth. 
                            </p>
                          </figcaption>
                          <img class="graph" src="assets/images/mobilenetbb.png">
                          <figcaption>
                            Looking closer to the average accuracy plotted against the number of estimators, a significant increase in accuracy is observed from 2 to 6 estimators, after which the rate of improvement slows down. An estimator value of 6 appears to be the optimal choice for conserving computational cost while still achieving good accuracy. However, if computational resources are not a concern, an estimator value of 14 may provide even better performance.  
                          </figcaption>
                         </figure>
                  
                <div style='border-bottom:3px solid lightgrey;'></div>
                <p></p>
                <h3> CNNs</h3>
                 <p></p>
                 <p> 
                  From the accuracy metric, we were able to record our model’s training and validation accuracy, which is reflected in the plot below.
                </p>
                <img class="graph" src="assets/images/cnn_accuracy.png">
                <img class="graph" src="assets/images/cnn_loss.png">
                <p>
                  Our classifier is highly volatile in its prediction accuracy, which is hovering around 40%. This likely means that our model did not converge properly. Further, the loss is too high.
                  Which means we are making numerous mistakes. This is likely due to several factors:                
                </p>
                <ul>
                  <li>
                    As this is an image classifier, our model would do better with more training and validation data. <1000 images is not enough to train on.
                  </li>
                  <li>
                    The bounding boxes are too big compared to some zoomed out images of the animal.
                  </li>
                  <li>
                    The animals are evolved to blend in with their surrounding for survival. Hence, the confusion between animal and background 
                    image is high. This is reflected in a 2 class classifier between just the penguins and turtle (plot attached below). This model 
                    has much higher and stable accuracy.
                  </li>
                </ul>
                <img class="graph" src="assets/images/cnn_twoclass_accuracy.png">
                <img class="graph" src="assets/images/cnn_twoclass_loss.png">
              </div>
            </div>
        </div>
        <div class="jumbotron jumbotron-fluid">
          <div class="container" style="background: #ffff">
              <div class="mt-0">
                <h2>Discussion</h2>
                <div style='border-bottom:3px solid lightgrey;'></div>
              </div>
              <div style="margin-top: 10px; padding: 20px;">
                <h3> Unsupervised </h3>
                <p>
                  By resizing the images and strictly analyzing the background color, we were able to improve the differentiation between images of turtles and penguins. When using edge detection, which helped represent features like the beaks, fins, and shells in the provided image did work so well as you can see in the above image. This suggested that the features between penguins and turtles might be too like rely solely on edge. Using Histogram of Gradients, we discovered penguins exhibit more uniform and gradual changes in gradients, while turtles had a more varied and complex gradient. Despite this information, a clear separation between the two classes was still not apparent. The most effective approach was the utilization of MobileNetV2 because it automatically distinguished specific patterns associated with turtles and penguins and used them as features. 
                </p>
                <p>
                  Due to the poor performance of edge detection and gradient distribution we decided to try incorporating these techniques with other ones like PCA. We also incorporated data augmentation, utilizing techniques such as funneling our images through a data frame and experimenting with trace breaks in code to explore alternative ideas. Our images from dataset are converted into data frames to integrate data analysis libraries, perform feature extraction, and help with data preprocessing. 
                </p>
                <p>
                  During our unsupervised learning analysis, we used Clustering, K-means, Gaussian mixture Models (GMM), and DBSCAN. Evaluating the clustering results provided insights into the quality of the clusters. The Silhouette coefficient, which measures cluster separation, seemed relatively low for all charts, indicating that clusters were overlapping with each other. The DBI metric supports the presence of overlapping clusters with it being high around 0.85. Looking at the NMI metric, which quantifies the similarity between predicted clusters and true labels, the score was relatively low. This suggests that the clusters have a limited similarity and our cluster algorithms struggled to capture underlying patterns accurately. Finally, Folkes-Mallow, which is used to evaluate the similarities between predicted clusters and true labels, demonstrated higher scores. Comparing the performance between GMM and K-Means, it seemed like K-Means did much better with its NMI mostly being larger than all the algorithms except Histogram of Gradients. In summary, our analysis revealed the challenge of achieving distinct and well separated clusters due to overlapping patterns.  
                </p> 
                <h3> Supervised </h3>
                <p>
                  During our project, we explored four supervised learning methods: decision trees, random forests, CNNs, and SVMs. The reason behind choosing CNNs was their unique capability to automatically learn hierarchical features from images, regardless of their position, making them ideal for image classification tasks. Decision trees were incorporated in our project due to their effectiveness with smaller datasets and their usefulness in feature importance analysis. Additionally, decision trees can be combined with other machine learning techniques like random forests, which we also employed. We opted for random forests mainly to address the concern of overfitting. Lastly, we used SVMs because of their robustness against overfitting, their ability to handle nonlinear data, and the few hyper parameters needed to tune.  
                </p>
                <p>
                  As for evaluation metrics we used accuracy and confusion matrix. The reason we used it is because confusion matrices work well with models with multiple classes. For example, in our project we have three classes: penguins, turtles, and backgrounds. Furthermore, they provide information on true positives, true negatives, and false positives, and false negatives. As for accuracy, it is simply used to serve as a baseline metric to assess a model’s performance.  
                </p>
                <p>
                  The results for our binary CNN classifier performed better than our multilclass with backgrounds. As spoken about, there reasons for this such as 
                  animal adaption and color schemes, however I still think there were errors in adjusting our data correctly which led to our results not yielding a high accuracy. 
                  It is entirely possible we used the worng optimizer or activations or had improper depth on the model. For these reasons CNN was not the best supervised learning method for our team. 
                </p>
                <p>
                  The supervised learning methods exhibited significant differences in their performance. SVMs accuracy stayed around 42% to 46%, which is comparatively lower than other methods and not much better than random guessing. As the accuracy for decision tree and random forest, it varied based on the selected feature. Notably, the Mobile Net Random Forests performed the best with an accuracy of 96.3%. Decision Trees performed slightly worse than Random Forests with a maximum accuracy achieved of 88.9%. As seen Random Forests tend to give better accuracy partially because it helps with overfitting in general. This is shown in our results as Random Forests performed better than Decision Trees. Lastly the method that we did last was CNN, which was expected to produce the best results because of the automatic feature learning, pooling layers, and hierarchical representations. However, whether it was an issue in code or by image formatting the multiclass network showed poor results among all types of activations. It seemed to top out at 26.1% each time.  
                </p>

                <h3> Final Thoughts </h3>
                <p>
                  Given more time to work on this project, we would like to do more exploration on this data. More data preprocessing and augmentation. Simple techniques such as zooms, rotations and flips might make a huge difference. Grayscale conversion and image inversion will give us more information about our misclassification, as per our project midpoint feedback (was pixel intensity the key?). 
                </p>
              </div>
            </div>
        </div>

        <div class="jumbotron jumbotron-fluid">
          <div class="container" style="background: #ffff">
              <div class="mt-0">
                <h2>References</h2>
                <div style='border-bottom:3px solid lightgrey;'></div>
              </div>
              <div style="margin-top: 10px; padding: 20px;">
                <a href="https://www.kaggle.com/datasets/abbymorgan/penguins-vs-turtles"
                >Kaggle Turtles and Penguins Dataset</a
              >
              <br />
              <a href="https://ieeexplore.ieee.org/abstract/document/9132851?casa_token=GeeAyY3ANu0AAAAA:4df-4yo2tQKZr5yTgExPcTzbx4RzWSUkYqooTc7ygn71H5q7BOMXPrhDW9TG9X_Bmg0LgT8unQ"
                >Image Classification using SVM and CNN</a>
              <br />
                <a href="https://www.nature.com/articles/s41467-022-27980-y">Perspectives in machine learning for wildlife conservation</a>
              </div>
            </div>
        </div>
              </div>
          </div>
        </div>
        </div>
      </article>
    </div>
    <footer>
      By Blake West, Derrick Yu, Eirene Lakshita, Lauren Fowler, John Binek
    </footer>
  </body>
</html>
